\chapter{Kompilierte Anfragepipelines}
\label{sec:pipelining}

In dieser Arbeit werden String-Vergleiche im Kontext des Query Compilers \emph{DogQC} für GPUs untersucht.
Dieser basiert auf dem Query Compiler \emph{HorseQC} \cite{Funke2018} und wurde für das erleichterte Testen von aktuellen Techniken vereinfacht.
DogQC erstellt aus einem gegebenen Anfrageplan mehrere Anfragepipelines, die für die Ausführung auf GPUs optimiert sind.
Als Grundlage für die späteren Codebeispiele werden hier die grundsätzliche Funktionsweise des Query Compilers erklärt und die Vorteile dieser Technik erläutert.

Klassische in-memory-Datenbanken wie \emph{MonetDB} arbeiten die Operatoren innerhalb eines Anfrageplans nacheinander ab, was auch \emph{Operator-At-A-Time} genannt wird \cite{Varga2018}.
Dabei wird für den gesamten Datensatz zunächst der erste Operator vollständig ausgeführt, bevor der gesamte Datensatz an den nächsten Operator weiter gegeben wird, bis schließlich der gesamte Anfrageplan abgearbeitet wurde.
Der Nachteil dieser Strategie besteht in einer besonders hohen Lese- und Schreiblast für die Zwischenergebnisse der Operatoren, da diese nach jeder Operation im Speicher materialisiert werden müssen.
Soll die Berechnung auf einer GPU erfolgen, kann es passieren, dass der begrenzte GPU-Speicher nicht ausreicht, um die Zwischenergebnisse zu speichern.
Somit werden während der Berechnung Transfers in den Hauptspeicher des Systems notwendig, um neue Blöcke der Tabelle nachzuladen.
Als Konsequenz entstehen massive Flaschenhälse durch die begrenzte Bandbreite.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\node (count) {\textsf{count(*)}};
		\node [below = 1cm of count] (join) {$\bowtie$};
		\node [below right = -0.3cm and -0.3cm of join, style={align=center}, font=\linespread{0.7}\selectfont] (join_text) {\scriptsize \textsf{orders.date=}\\ \scriptsize \textsf{dates.key}};
		
		\node [below left = 1cm and 1cm of join] (select_id) {$\sigma$};
		\node [below right = -0.3cm and -0.3cm of select_id] (select_id_text) {\scriptsize \textsf{year=2019}};
		\node [below = 1cm of select_id] (orders) {\textsf{dates}};
		
		\node [below right = 1cm and 1cm of join] (select_year) {$\sigma$};
		\node [below right = -0.3cm and -0.3cm of select_year] (select_year_text) {\scriptsize \textsf{prio=1}};
		\node [below = 1cm of select_year] (dates) {\textsf{orders}};
		
		\draw [thick, shorten > = 0.2cm, shorten < = 0.2cm] (count) -- (join);
		\draw [thick, shorten > = 0.2cm, shorten < = 0.4cm] (join) -- (select_id);
		\draw [thick, shorten > = 0.2cm, shorten < = 0.2cm] (select_id) -- (orders);
		\draw [thick, shorten > = 0.2cm, shorten < = 0.7cm] (join) -- (select_year);
		\draw [thick, shorten > = 0.2cm, shorten < = 0.2cm] (select_year) -- (dates);
		
		\coordinate [below left = 0cm and 0cm of orders] (l1);
		\coordinate [above = 1.7cm of l1] (l2);
		\coordinate [below left = 0.05cm and 0.3cm of join] (l3);
		\coordinate [below left = 0.4cm and 0cm of join] (l4);
		\coordinate [below right = 0cm and 1.3cm of select_id] (l5);
		\coordinate [below right = 0cm and 0cm of orders] (l6);
		
		\draw[thick, dashed, rounded corners=0.2cm]
		(l1) -- (l2) -- (l3) -- (l4) -- (l5) -- (l6) -- cycle;
		
		\coordinate [below left = 0cm and 0cm of dates] (r1);
		\coordinate [above = 1.3cm of r1] (r2);
		\coordinate [above left = -0.1cm and 0.5cm of join] (r3);
		\coordinate [above left = 0cm and 0cm of count] (r4);
		\coordinate [above right = 0cm and 0cm of count] (r5);
		\coordinate [below right = -0.1cm and 1cm of select_year] (r6);
		\coordinate [below right = 0cm and 0cm of dates] (r7);
		
		\draw[thick, dashed, rounded corners=0.2cm]
		(r1) -- (r2) -- (r3) -- (r4) -- (r5) -- (r6) -- (r7) -- cycle;
		
		\node [above left = 1cm and 0cm of select_id] (ht_build) {\textsf{HT build}};
		\coordinate [below left = 0.2cm and 0.6cm of join] (ht_build_arrow_end);
		
		\draw [thick,->] (ht_build) -- (ht_build_arrow_end);
				
	\end{tikzpicture}
	\caption{Beispielplan mit eingezeichneten Pipelines}
	\label{pipeline_example_plan}
\end{figure}

Das Pipelining-Prinzip, welches bei dem vorgestellten Query Compiler zum Einsatz kommt, legt fest, dass der Anfrageplan in Pipelines aufgeteilt wird, die von den Tupeln immer vollständig durchlaufen werden, bevor das Ergebnis in den Speicher geschrieben wird.
Dieses Vorgehen wird \emph{Tuple-At-A-Time} genannt.
Die Operatoren innerhalb des Anfrageplans aus Abbildung \ref{pipeline_example_plan} werden zu zwei Pipelines zusammengefasst.
In der linken Pipeline wird die \textsf{dates}-Relation gelesen, die Selektion ausgeführt und die Hashtabelle für den Join berechnet.
Die rechte Pipeline fasst das Lesen der \textsf{orders}-Relation, die Selektion, die Probe-Operation der Hashtabelle und das Zählen der Ergebnisse zusammen.
Technisch werden die Operationen innerhalb der Pipeline zu einem einzigen Operator verschmolzen, indem der Query Compiler für jede Pipeline einen eigenständigen Kernel generiert, welcher mit der CUDA-Schnittstelle ausgeführt werden kann.
Der Vorteil des Pipelinings besteht darin, dass die vollständigen Ergebnismengen nicht nach jedem Operator im Speicher materialisiert werden müssen, sondern die einzelnen Tupel stets in den GPU-Registern vorgehalten werden können, bis diese fertig verarbeitet wurden.

Klassische \emph{Tuple-At-A-Time}-Ausführungsmuster wie das \emph{Volcano Iterator Model} definieren Operatoren als eigenständige Bausteine mit festgelegten Schnittstellen, die von den Tupeln nacheinander durchlaufen werden \cite{Graefe1994}.
Im Gegensatz dazu werden die Operatoren bei kompilierten Anfragepipelines zu einem Operator verschmolzen, wodurch Funktionsaufrufe entfallen und die Tupel in einem \emph{tight loop} \cite{Neumann2011} verarbeitet werden.

\begin{figure}[h]
	\begin{lstlisting}[language=MyC++,
	linebackgroundcolor={%
	\ifnum\value{lstnumber}>24
		\ifnum\value{lstnumber}<28
			\color{design1!35}
		\fi
	\fi
	\ifnum\value{lstnumber}>28
		\ifnum\value{lstnumber}<33
			\color{design2!35}
		\fi
	\fi
	\ifnum\value{lstnumber}>33
		\ifnum\value{lstnumber}<38
			\color{design3!35}
		\fi
	\fi
	}]
	__global__
	void joinProbePipeline(
		int *orders_prio,              	// priority attribute of orders table
		int *orders_date,               // date attribute of orders table
		unique_ht *hashtable_date_key,  // hashtable from other pipeline
		int *number_of_matches) {       // return value
		
		// global index of the current thread,
		// used as the iterator in this case
		unsigned loop_var = ((blockIdx.x * blockDim.x) + threadIdx.x);
		
		// offset for the next element to be computed
		unsigned step = (blockDim.x * gridDim.x);
		
		bool active = true;
		bool flush_pipeline = false;
		while(!flush_pipeline) {
		
			// element index must not be higher than number of tuples
			active = loop_var < TUPLE_COUNT_ORDERS;
			
			// break computation when every line is finished and therefore inactive
			flush_pipeline = !__ballot_sync(ALL_LANES, active);
			
			// selection
			if (active)
				active = orders_prio[loop_var] == 1;
			
			// hash join probe
			if (active)
				active = hashProbeUnique(hashtable_date_key, HASHTABLE_SIZE, 
					hash(orders_date[loop_var]))
			
			// count and write
			numProj = __popc(__ballot_sync(ALL_LANES, active))
			if (threadIdx.x % 32 == 0)
				atomicAdd(number_of_matches, numProj);
			
			loop_var += step;
		}
	}
	
	\end{lstlisting}
	\caption{Generierter Kernel für den Beispielplan}
	\label{pipelining_example_code}
\end{figure}

In Abbildung \ref{pipelining_example_code} wird der Code vorgestellt, den der Query Compiler für die rechte Pipeline aus dem in Abbildung \ref{pipeline_example_plan} dargestellten Anfrageplan generiert.
Hier ist zu erkennen, dass die drei Operationen innerhalb eines Kernels zusammengefasst wurden, wodurch eine Pipeline entsteht.
Um jedem Thread eine Menge von Tupeln zuweisen zu können, wird zunächst der globale Index des aktuellen Threads innerhalb des Grids berechnet, damit dieser als Schleifenindex \texttt{loop\_var} verwendet werden kann.
Anschließend wird über alle Elemente aus dem Datensatz iteriert, für die der aktuelle Thread zuständig ist.
Die Variable \texttt{active} zeigt im Algorithmus an, ob der aktuelle Thread aktiv läuft oder nur darauf wartet, dass die anderen Threads aus seinem Warp ihre Berechnung abschließen.

In einem Schleifendurchlauf, bei dem das Element mit dem Index \texttt{loop\_var} untersucht wird, wird zunächst die blau markierte Selektion ausgeführt.
Ist diese fehlgeschlagen, da die Priorität der Bestellung nicht bei 1 liegt, wird der Thread deaktiviert und im weiteren Verlauf nicht mehr beachtet, bis er ein neues Tupel erhält.
Hat sich das Tupel qualifiziert, folgt darauf der Hash Probe, welcher in grün dargestellt ist.
Dabei wird das Tupel in der übergebenen Hashtabelle \texttt{hashtable\_date\_key} gesucht und dementsprechend wieder die \texttt{active}-Variable angepasst.
Schließlich wird vom Query Compiler noch das Zählen der Ergebnisse umgesetzt, welches hier in rot hervorgehoben ist.
Dabei wird mithilfe der Synchronisierungsoperation \texttt{\_\_ballot\_sync} die Anzahl der aktiven Lanes gezählt, welche jeweils ein Element des Ergebnisses repräsentieren.
Diese Anzahl wird daraufhin vom ersten Thread innerhalb des Warps auf das Ergebnis addiert. 

Nach der Auswertung der gesamten Pipeline von Operationen für das untersuchte Tupel wird der Index um die vorher definierte Schrittweite erhöht, sodass im nächsten Schleifendurchlauf das nächste Tupel untersucht wird.
Falls der neu gewählte Index hinter dem Ende der Daten liegt, hat der aktuelle Thread seine Arbeit vollständig abgeschlossen und er wird nicht mehr benötigt, sodass die \texttt{active}-Variable in Zeile 20 auf \texttt{false} gesetzt wird.
Wird anschließend mithilfe der \texttt{\_\_ballot\_sync}-Methode festgestellt, dass sämtliche Lanes inaktiv sind, ist der Datensatz vollständig durchlaufen worden und die Berechnung kann abgeschlossen werden.
Das parallele Abarbeiten eines Warps und das Synchronhalten aller Lanes wird \emph{warp synchronous programming} genannt \cite{Lin2018}.

Durch die Untersuchung des hier vorgestellten Verfahrens der \emph{Tuple-At-A-Time}-Ver\-ar\-bei\-tung wird klar, dass dieses einen großen Vorteil bei der effizienten Nutzung von schnellem Speicher gegenüber der \emph{Operator-At-A-Time} Verarbeitung bietet.
Zwischenergebnisse müssen nicht materialisiert werden, weshalb Tupel in Registern oder Caches vorgehalten werden können und die Operationen für das explizite Materialisieren entfallen.

Im Gegensatz zu der \emph{Operator-At-A-Time}-Technik lässt sich das Pipelining allerdings nicht so einfach auf eine parallele Verarbeitung mit GPUs übertragen.
Wird jedem Thread in einem Warp ein Tupel übergeben, kann es beispielsweise passieren, dass dieses bei einer Selektion aus der Ergebnisrelation heraus fällt, somit im weiteren Verlauf nicht weiter beachtet werden muss und die entsprechende Lane inaktiv wird.
Dieses Problem wird in Kapitel \ref{sec:unterauslastung_problem} aufgegriffen und in Kapitel \ref{sec:equals_lane_refill} wird ein Lösungsvorschlag dafür vorgestellt.